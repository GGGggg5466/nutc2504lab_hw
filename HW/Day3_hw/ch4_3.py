import os
from dotenv import load_dotenv

from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

load_dotenv()

BASE_URL = os.getenv("OPENAI_BASE_URL")
API_KEY  = os.getenv("OPENAI_API_KEY")
MODEL    = os.getenv("OPENAI_MODEL")

llm = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL,
    temperature=0
)

# =========================
# ch4-3 Toolï¼šç§‘æŠ€æ–‡ç« æ‘˜è¦å·¥å…·
# =========================
@tool
def generate_tech_summary(article_content: str) -> str:
    """
    ç§‘æŠ€æ–‡ç« å°ˆç”¨æ‘˜è¦ç”Ÿæˆå·¥å…·ã€‚

    ã€åˆ¤æ–·é‚è¼¯ã€‘
    1. åªæœ‰ç•¶è¼¸å…¥å…§å®¹å±¬æ–¼ã€Œç§‘æŠ€ã€ã€ã€Œç¨‹å¼è¨­è¨ˆã€ã€ã€ŒAIã€ã€ã€Œè»Ÿé«”å·¥ç¨‹ã€æˆ–ã€ŒIT æŠ€è¡“ã€é ˜åŸŸæ™‚ï¼Œæ‰ä½¿ç”¨æ­¤å·¥å…·ã€‚
    2. å¦‚æœå…§å®¹æ˜¯ã€Œé–’èŠã€ã€ã€Œé£Ÿè­œã€ã€ã€Œå¤©æ°£ã€ã€ã€Œæ—¥å¸¸æ—¥è¨˜ã€ç­‰éæŠ€è¡“å…§å®¹ï¼Œè«‹å‹¿ä½¿ç”¨æ­¤å·¥å…·ã€‚

    åŠŸèƒ½ï¼šå°‡è¼¸å…¥çš„æŠ€è¡“æ–‡ç« æ­¸ç´å‡º 3 å€‹é‡é»ï¼ˆKey Takeawaysï¼‰ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„ç§‘æŠ€ä¸»ç·¨ã€‚è«‹å°‡è¼¸å…¥çš„æŠ€è¡“æ–‡ç« å…§å®¹ï¼Œç²¾ç°¡åœ°æ­¸ç´å‡º 3 å€‹é—œéµé‡é»ï¼ˆKey Takeawaysï¼‰ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºï¼Œæ¢åˆ—æ¸…æ¥šã€‚"),
        ("user", "{text}")
    ])

    chain = prompt | llm | StrOutputParser()
    result = chain.invoke({"text": article_content})
    return result


# =========================
# ch4-3 Routerï¼šæ±ºå®šè¦ä¸è¦ç”¨å·¥å…·
# =========================
llm_with_tools = llm.bind_tools([generate_tech_summary])

router_prompt = ChatPromptTemplate.from_messages([
    ("user", "{input}")
])

def main():
    print("=== ch4-3 interactive router ===")
    print("è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼šè‹¥æ˜¯ç§‘æŠ€æ–‡ç« æœƒè‡ªå‹•æ‘˜è¦ï¼›è‹¥ä¸æ˜¯ç§‘æŠ€æ–‡ç« å°±ç›´æ¥å›è¦†ã€‚")
    print("è¼¸å…¥ exit æˆ– q é›¢é–‹ã€‚\n")

    while True:
        user_input = input("User: ").strip()
        if user_input.lower() in ["exit", "q"]:
            print("Bye!")
            break

        chain = router_prompt | llm_with_tools
        ai_msg = chain.invoke({"input": user_input})

        tool_calls = getattr(ai_msg, "tool_calls", None)

        if tool_calls:
            print("âœ… [æ±ºç­–] åˆ¤æ–·ç‚ºç§‘æŠ€æ–‡ç« ")
            tool_args = tool_calls[0]["args"]

            final_result = generate_tech_summary.invoke(tool_args)

            print(f"ğŸ“Œ [åŸ·è¡Œçµæœ]:\n{final_result}\n")
        else:
            print("âŒ [æ±ºç­–] åˆ¤æ–·ç‚ºé–’èŠ/éç§‘æŠ€æ–‡ç« ï¼Œç›´æ¥å›ç­”")
            print(f"ğŸ’¬ [AI å›æ‡‰]: {ai_msg.content}\n")


if __name__ == "__main__":
    main()
