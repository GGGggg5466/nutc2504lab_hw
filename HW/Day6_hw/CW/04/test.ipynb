{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93f68508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qdrant collection ready: cw04_hybrid_rerank\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"cw04_hybrid_rerank\"\n",
    "DENSE_VECTOR_SIZE = 4096\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "# 如果已存在，先刪掉重建（避免測試時混到舊資料）\n",
    "if client.collection_exists(COLLECTION_NAME):\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=DENSE_VECTOR_SIZE,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"✅ Qdrant collection ready: {COLLECTION_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f849fd",
   "metadata": {},
   "source": [
    "# Step 2-A：讀取 chunks_semantic.jsonl 成為 chunks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0669e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded chunks: 36\n",
      "Example keys: ['id', 'source', 'chunk_id', 'start_token', 'end_token', 'text', 'source_path', 'tokenizer', 'method', 'max_tokens', 'min_tokens', 'sim_threshold']\n",
      "Example: data_01.txt::0 data_01.txt 今天是2月2日星期一，台中市迎來了一個涼爽而舒適的清晨。\n",
      "凌晨時分，氣溫約在攝氏16度左右，空氣中帶著冬季特有的清新感。\n",
      "隨著太陽逐漸升起，天空呈現出淡藍色的基\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "CHUNKS_JSONL = Path(\"./chunks_semantic.jsonl\")\n",
    "\n",
    "chunks = []\n",
    "with CHUNKS_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "print(f\"✅ Loaded chunks: {len(chunks)}\")\n",
    "print(\"Example keys:\", list(chunks[0].keys()))\n",
    "print(\"Example:\", chunks[0][\"id\"], chunks[0][\"source\"], chunks[0][\"text\"][:80])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf13985",
   "metadata": {},
   "source": [
    "# Step 2-B：批次呼叫 Embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ef92df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings: 36\n",
      "Dim: 4096\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import requests\n",
    "\n",
    "EMBED_API_URL = \"https://ws-04.wade0426.me/embed\"\n",
    "TASK_DESCRIPTION = \"檢索技術文件\"\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    r = requests.post(\n",
    "        EMBED_API_URL,\n",
    "        json={\n",
    "            \"texts\": texts,\n",
    "            \"task_description\": TASK_DESCRIPTION,\n",
    "            \"normalize\": True,\n",
    "        },\n",
    "        timeout=120\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"embeddings\"]\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "BATCH = 16\n",
    "embeddings = []\n",
    "for i in range(0, len(texts), BATCH):\n",
    "    batch_texts = texts[i:i+BATCH]\n",
    "    embeddings.extend(get_embeddings(batch_texts))\n",
    "\n",
    "print(f\"✅ Embeddings: {len(embeddings)}\")\n",
    "print(\"Dim:\", len(embeddings[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bba917",
   "metadata": {},
   "source": [
    "# Step 2-C：Upsert 到 Qdrant（dense + sparse/BM25）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea435a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted 36 points into cw04_hybrid_rerank\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from qdrant_client import QdrantClient, models\n",
    "import uuid\n",
    "\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"cw04_hybrid_rerank\"\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "points = []\n",
    "for c, emb in zip(chunks, embeddings):\n",
    "    qdrant_id = uuid.uuid4().hex  # ✅ 合法 id（UUID）\n",
    "\n",
    "    points.append(\n",
    "        models.PointStruct(\n",
    "            id=qdrant_id,\n",
    "            vector={\n",
    "                \"dense\": emb,\n",
    "                \"sparse\": models.Document(text=c[\"text\"], model=\"Qdrant/bm25\"),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": c[\"text\"],\n",
    "                \"source_file\": c.get(\"source\", \"\"),\n",
    "                \"chunk_id\": c.get(\"chunk_id\", None),\n",
    "                \"start_token\": c.get(\"start_token\", None),\n",
    "                \"end_token\": c.get(\"end_token\", None),\n",
    "                \"chunk_uid\": c.get(\"id\", \"\"),  # ✅ 把 data_01.txt::0 留下來\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "print(f\"✅ Upserted {len(points)} points into {COLLECTION_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdcc61",
   "metadata": {},
   "source": [
    "# Step 3-A：先寫一個 Hybrid Search 函式（只負責拿候選 chunks）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c988cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from qdrant_client import QdrantClient, models\n",
    "import requests\n",
    "\n",
    "QDRANT_URL = \"http://localhost:6333\"\n",
    "COLLECTION_NAME = \"cw04_hybrid_rerank\"\n",
    "\n",
    "EMBED_API_URL = \"https://ws-04.wade0426.me/embed\"\n",
    "TASK_DESCRIPTION = \"檢索技術文件\"\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL)\n",
    "\n",
    "def embed_query(query: str):\n",
    "    r = requests.post(\n",
    "        EMBED_API_URL,\n",
    "        json={\"texts\": [query], \"task_description\": TASK_DESCRIPTION, \"normalize\": True},\n",
    "        timeout=60\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"embeddings\"][0]\n",
    "\n",
    "def hybrid_retrieve(query: str, initial_limit: int = 20):\n",
    "    q_dense = embed_query(query)\n",
    "\n",
    "    res = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        # 兩路 prefetch：sparse(BM25) + dense(embedding)\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(text=query, model=\"Qdrant/bm25\"),\n",
    "                using=\"sparse\",\n",
    "                limit=initial_limit,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=q_dense,\n",
    "                using=\"dense\",\n",
    "                limit=initial_limit,\n",
    "            ),\n",
    "        ],\n",
    "        # 用 RRF 融合（學長示範的融合方式）\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=initial_limit,\n",
    "    )\n",
    "\n",
    "    hits = []\n",
    "    for p in res.points:\n",
    "        payload = p.payload or {}\n",
    "        hits.append({\n",
    "            \"point_id\": p.id,\n",
    "            \"score\": p.score,\n",
    "            \"text\": payload.get(\"text\", \"\"),\n",
    "            \"source_file\": payload.get(\"source_file\", \"\"),\n",
    "            \"chunk_uid\": payload.get(\"chunk_uid\", \"\"),\n",
    "        })\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76365ccf",
   "metadata": {},
   "source": [
    "# Step 3-B：先用一題測試（確認 retrieval 正常）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7145fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 10 candidates\n",
      "\n",
      "[1] score=0.5000 source=data_05.txt chunk_uid=data_05.txt::5\n",
      "LiteRT的訴求是把差異整合進同一套機制，讓開發者以較一致的方法啟用NPU加速，並在裝置不支援或條件不足時，仍能自動改用GPU或CPU維持可用性。\n",
      "\n",
      "[2] score=0.3333 source=data_05.txt chunk_uid=data_05.txt::1\n",
      "LiteRT技術堆疊從TensorFlow Lite的基礎往前推進，過去TensorFlow Lite主要服務傳統機器學習推論，而LiteRT的定位則是接手新一代裝置端AI需求，包含更廣泛的硬體加速與跨平臺部署。\n",
      "\n",
      "[3] score=0.2500 source=data_05.txt chunk_uid=data_05.txt::0\n",
      "Google更新裝置端推論框架LiteRT，宣布在Google I/O 2025預告的進階硬體加速能力，已正式納入LiteRT產品堆疊並對開發者開放。\n",
      "這次更新把GPU與NPU的加速流程補齊，其中GPU支援從I/O 2025當時先在Android導入的路徑，擴大到Android、iOS、macOS、Windows、Li\n",
      "\n",
      "[4] score=0.2000 source=data_05.txt chunk_uid=data_05.txt::2\n",
      "LiteRT的GPU加速支援範圍涵蓋Android、iOS、macOS、Windows、Linux與Web，並透過ML Drift這套下一代GPU引擎，串接OpenCL、OpenGL、Metal與WebGPU等後端。\n",
      "在Android裝置上，LiteRT會在可用時優先採用OpenCL以取得較高效能，必要時退回OpenG\n",
      "\n",
      "[5] score=0.1667 source=data_05.txt chunk_uid=data_05.txt::7\n",
      "同時，LiteRT提供AOT與裝置端JIT兩種編譯策略，讓開發者在啟動速度與首次執行成本之間做取捨。\n",
      "LiteRT維持以.tflite格式作為跨平臺部署的共同基礎，開發者可利用PyTorch、TensorFlow與JAX等常見訓練框架轉換模型，讓不同來源的模型都能接上同一套裝置端推論與硬體加速能力，降低因訓練框架不同而\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "test_query = \"LiteRT 更新有哪些重點？\"\n",
    "hits = hybrid_retrieve(test_query, initial_limit=10)\n",
    "\n",
    "print(f\"✅ Retrieved {len(hits)} candidates\")\n",
    "for i, h in enumerate(hits[:5], 1):\n",
    "    print(f\"\\n[{i}] score={h['score']:.4f} source={h['source_file']} chunk_uid={h['chunk_uid']}\")\n",
    "    print(h[\"text\"][:160])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085c57d",
   "metadata": {},
   "source": [
    "# Step 4-A：載入 Reranker 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09a381ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reranker loaded. CUDA: True\n",
      "Model path: /home/randy/Qwen3-Reranker-0.6B\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# TODO: 換成你剛剛在 WSL 終端機確認存在的那條路徑\n",
    "MODEL_DIR = \"/home/randy/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "tokenizer_rerank = AutoTokenizer.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "model_rerank = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model_rerank.eval()\n",
    "\n",
    "print(\"✅ Reranker loaded. CUDA:\", torch.cuda.is_available())\n",
    "print(\"Model path:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a6028",
   "metadata": {},
   "source": [
    "# Step 4-B：定義 ReRank 打分函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22832fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import math\n",
    "\n",
    "@torch.inference_mode()\n",
    "def rerank_score_yes_prob(query: str, doc: str) -> float:\n",
    "    prompt = (\n",
    "        \"You are a relevance judge.\\n\"\n",
    "        \"Given a query and a document, answer 'yes' if the document is relevant to the query, otherwise answer 'no'.\\n\\n\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"Document: {doc}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer_rerank(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model_rerank.device) for k, v in inputs.items()}\n",
    "\n",
    "    out = model_rerank(**inputs)\n",
    "    logits = out.logits[0, -1]  # 最後一個 token 的 logits\n",
    "\n",
    "    # 取得 \" yes\" / \" no\" 的 token id（包含前導空白比較常見）\n",
    "    yes_ids = tokenizer_rerank.encode(\" yes\", add_special_tokens=False)\n",
    "    no_ids  = tokenizer_rerank.encode(\" no\", add_special_tokens=False)\n",
    "\n",
    "    # 保守處理：取第一個 token id（通常就夠用）\n",
    "    yes_id = yes_ids[0]\n",
    "    no_id = no_ids[0]\n",
    "\n",
    "    yes_logit = logits[yes_id].float().item()\n",
    "    no_logit  = logits[no_id].float().item()\n",
    "\n",
    "    # softmax over {yes, no}\n",
    "    m = max(yes_logit, no_logit)\n",
    "    yes_p = math.exp(yes_logit - m) / (math.exp(yes_logit - m) + math.exp(no_logit - m))\n",
    "    return float(yes_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c1a11",
   "metadata": {},
   "source": [
    "# Step 4-C：對 Step 3 的 hits 做 rerank，輸出 Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c46b98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rerank Top-5:\n",
      "\n",
      "[1] rerank=0.9986 (rrf=0.2500) source=data_05.txt uid=data_05.txt::0\n",
      "Google更新裝置端推論框架LiteRT，宣布在Google I/O 2025預告的進階硬體加速能力，已正式納入LiteRT產品堆疊並對開發者開放。\n",
      "這次更新把GPU與NPU的加速流程補齊，其中GPU支援從I/O 2025當時先在Android導入的路徑，擴大到Android、iOS、macOS、Windows、Li\n",
      "\n",
      "[2] rerank=0.9951 (rrf=0.1667) source=data_05.txt uid=data_05.txt::7\n",
      "同時，LiteRT提供AOT與裝置端JIT兩種編譯策略，讓開發者在啟動速度與首次執行成本之間做取捨。\n",
      "LiteRT維持以.tflite格式作為跨平臺部署的共同基礎，開發者可利用PyTorch、TensorFlow與JAX等常見訓練框架轉換模型，讓不同來源的模型都能接上同一套裝置端推論與硬體加速能力，降低因訓練框架不同而\n",
      "\n",
      "[3] rerank=0.9868 (rrf=0.3333) source=data_05.txt uid=data_05.txt::1\n",
      "LiteRT技術堆疊從TensorFlow Lite的基礎往前推進，過去TensorFlow Lite主要服務傳統機器學習推論，而LiteRT的定位則是接手新一代裝置端AI需求，包含更廣泛的硬體加速與跨平臺部署。\n",
      "\n",
      "[4] rerank=0.9797 (rrf=0.5000) source=data_05.txt uid=data_05.txt::5\n",
      "LiteRT的訴求是把差異整合進同一套機制，讓開發者以較一致的方法啟用NPU加速，並在裝置不支援或條件不足時，仍能自動改用GPU或CPU維持可用性。\n",
      "\n",
      "[5] rerank=0.9325 (rrf=0.1429) source=data_05.txt uid=data_05.txt::3\n",
      "Google表示，在多種模型的平均情境下，LiteRT的GPU效能平均約比既有的TensorFlow Lite GPU委派快1.4倍。\n",
      "LiteRT的效能改善，來自推論從輸入到輸出的整體等待時間縮短，Google讓裝置端推論更少依賴CPU執行額外的等待與資料處理，並降低資料在不同硬體之間搬移時造成的延遲。\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def rerank_hits(query: str, hits: list, final_top_k: int = 5):\n",
    "    scored = []\n",
    "    for h in hits:\n",
    "        s = rerank_score_yes_prob(query, h[\"text\"])\n",
    "        scored.append({**h, \"rerank_score\": s})\n",
    "    scored.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return scored[:final_top_k], scored\n",
    "\n",
    "# 用你剛剛 Step 3 的同一題測試\n",
    "query = test_query  # \"LiteRT 更新有哪些重點？\"\n",
    "topk, all_scored = rerank_hits(query, hits, final_top_k=5)\n",
    "\n",
    "print(\"✅ Rerank Top-5:\")\n",
    "for i, h in enumerate(topk, 1):\n",
    "    print(f\"\\n[{i}] rerank={h['rerank_score']:.4f} (rrf={h['score']:.4f}) source={h['source_file']} uid={h['chunk_uid']}\")\n",
    "    print(h[\"text\"][:160])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93d204",
   "metadata": {},
   "source": [
    "# Step 5-A：讀取 Prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da826311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompt loaded. length = 241\n",
      "你是一位專業的知識庫助手。請嚴格根據以下提供的【參考資訊】來回答用戶的問題。\n",
      "\n",
      "### 規則：\n",
      "1. **必須**只依賴【參考資訊】中的內容回答。\n",
      "2. 如果【參考資訊】中沒有足夠的資訊來回答問題，請直接回答：「抱歉，根據目前的資料庫，我無法回答這個問題。」，**絕對不要**憑空捏造或使用你的外部知識。\n",
      "3. 回答應簡潔、準確且專業。\n",
      "\n",
      "### 參考資訊：\n",
      "\"\"\"\n",
      "{{context_str}}\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from docx import Document\n",
    "\n",
    "PROMPT_DOCX_PATH = \"Prompt.txt.docx\"  # 若你檔名不同就改這裡\n",
    "\n",
    "doc = Document(PROMPT_DOCX_PATH)\n",
    "PROMPT_TEMPLATE = \"\\n\".join([p.text for p in doc.paragraphs]).strip()\n",
    "\n",
    "print(\"✅ Prompt loaded. length =\", len(PROMPT_TEMPLATE))\n",
    "print(PROMPT_TEMPLATE[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30eac0",
   "metadata": {},
   "source": [
    "# Step 5-B：把 Top-K chunks 組成 context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0db0d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Context built. chars = 966\n",
      "Chosen source_file: data_05.txt\n",
      "[1] source_file=data_05.txt chunk_uid=data_05.txt::0\n",
      "Google更新裝置端推論框架LiteRT，宣布在Google I/O 2025預告的進階硬體加速能力，已正式納入LiteRT產品堆疊並對開發者開放。\n",
      "這次更新把GPU與NPU的加速流程補齊，其中GPU支援從I/O 2025當時先在Android導入的路徑，擴大到Android、iOS、macOS、Windows、Linux與Web，讓裝置端AI推論在行動端、桌面與網頁之間更一致。\n",
      "\n",
      "[2] source_file=data_05.txt chunk_uid=data_05.txt::7\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def build_context(topk_hits):\n",
    "    blocks = []\n",
    "    for i, h in enumerate(topk_hits, 1):\n",
    "        blocks.append(\n",
    "            f\"[{i}] source_file={h['source_file']} chunk_uid={h.get('chunk_uid','')}\\n{h['text']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "context_str = build_context(topk)\n",
    "chosen_source_file = topk[0][\"source_file\"] if topk else \"\"\n",
    "\n",
    "print(\"✅ Context built. chars =\", len(context_str))\n",
    "print(\"Chosen source_file:\", chosen_source_file)\n",
    "print(context_str[:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066159be",
   "metadata": {},
   "source": [
    "# Step 5-C：呼叫 LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc1dcdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      "\n",
      "LiteRT 更新的重點包括：\n",
      "\n",
      "1. **硬體加速擴充**  \n",
      "   - 完成 GPU 與 NPU 的加速流程，支援平台從 Android 擴展至 iOS、macOS、Windows、Linux 與 Web，讓裝置端 AI 推論在行動、桌面與網頁間更一致。\n",
      "\n",
      "2. **編譯策略**  \n",
      "   - 同時提供 **AOT（Ahead‑Of‑Time）** 與 **裝置端 JIT（Just‑In‑Time）** 兩種編譯方式，讓開發者可在啟動速度與首次執行成本之間自行取捨。\n",
      "\n",
      "3. **跨框架模型部署**  \n",
      "   - 仍以 **.tflite** 為跨平台部署的共同基礎，支援從 PyTorch、TensorFlow、JAX 等常見訓練框架轉換模型，降低因訓練框架不同而產生的部署分歧。\n",
      "\n",
      "4. **統一加速機制與自動回退**  \n",
      "   - 以單一機制整合差異，讓開發者以一致方式啟用 NPU 加速；若裝置不支援或條件不足，系統會自動回退至 GPU 或 CPU，確保可用性。\n",
      "\n",
      "5. **效能提升**  \n",
      "   - 在多種模型的平均情境下，LiteRT 的 GPU 效能約比既有的 TensorFlow Lite GPU 委派快 **1.4 倍**。  \n",
      "   - 效能改善來自整體推論等待\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import requests\n",
    "\n",
    "LLM_API_URL = \"https://ws-03.wade0426.me/v1/chat/completions\"\n",
    "LLM_MODEL = \"/models/gpt-oss-120b\"\n",
    "\n",
    "def render_prompt(template: str, context_str: str, query_str: str) -> str:\n",
    "    return (\n",
    "        template\n",
    "        .replace(\"{{context_str}}\", context_str)\n",
    "        .replace(\"{{query_str}}\", query_str)\n",
    "    )\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0, max_tokens: int = 512) -> str:\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(LLM_API_URL, json=payload, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "final_prompt = render_prompt(PROMPT_TEMPLATE, context_str=context_str, query_str=query)\n",
    "\n",
    "answer = call_llm(final_prompt, temperature=0.0, max_tokens=512)\n",
    "\n",
    "print(\"✅ Answer:\\n\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70bf21",
   "metadata": {},
   "source": [
    "# Step 6-A — 讀取 questions.csv，確認欄位與建立輸出欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ca78da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded questions.csv rows = 10\n",
      "Columns: ['題目_ID', '題目', '標準答案', '來源文件', '模型回答', '命中來源文件']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>題目_ID</th>\n",
       "      <th>題目</th>\n",
       "      <th>標準答案</th>\n",
       "      <th>來源文件</th>\n",
       "      <th>模型回答</th>\n",
       "      <th>命中來源文件</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>文中提到台中市2月2日當天的氣溫變化範圍為何？</td>\n",
       "      <td>凌晨約攝氏16度左右，白天最高溫約23度。</td>\n",
       "      <td>data_01.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Google Cloud 推出的 N4A 虛擬機器採用哪一款處理器？</td>\n",
       "      <td>Google Axion 處理器（Arm 架構）。</td>\n",
       "      <td>data_04.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>根據林氏璧的指出，日本這波流感主要流行哪兩種病毒型別？其占比為何？</td>\n",
       "      <td>以A型H3為主（占74%），混著B型流感（占26%）。</td>\n",
       "      <td>data_03.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   題目_ID                                  題目                         標準答案  \\\n",
       "0      1             文中提到台中市2月2日當天的氣溫變化範圍為何？        凌晨約攝氏16度左右，白天最高溫約23度。   \n",
       "1      2  Google Cloud 推出的 N4A 虛擬機器採用哪一款處理器？    Google Axion 處理器（Arm 架構）。   \n",
       "2      3   根據林氏璧的指出，日本這波流感主要流行哪兩種病毒型別？其占比為何？  以A型H3為主（占74%），混著B型流感（占26%）。   \n",
       "\n",
       "          來源文件 模型回答 命中來源文件  \n",
       "0  data_01.txt              \n",
       "1  data_04.txt              \n",
       "2  data_03.txt              "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "QUESTIONS_PATH = Path(\"./questions_answer.csv\")\n",
    "df = pd.read_csv(QUESTIONS_PATH)\n",
    "\n",
    "required_cols = [\"題目_ID\", \"題目\", \"標準答案\", \"來源文件\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "assert not missing, f\"❌ questions.csv 缺少欄位: {missing}\"\n",
    "\n",
    "# 建議新增輸出欄位（不破壞原本資料）\n",
    "if \"模型回答\" not in df.columns:\n",
    "    df[\"模型回答\"] = \"\"\n",
    "if \"命中來源文件\" not in df.columns:\n",
    "    df[\"命中來源文件\"] = \"\"\n",
    "\n",
    "print(\"✅ Loaded questions.csv rows =\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e1d94",
   "metadata": {},
   "source": [
    "# Step 6-B — 定義「單題完整流程」：Hybrid → ReRank → LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b8b91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import time\n",
    "\n",
    "# ---- 你前面已經有的函式/物件，這裡假設已存在： ----\n",
    "# client, COLLECTION_NAME\n",
    "# hybrid_retrieve(query, initial_limit)\n",
    "# rerank_hits(query, hits, final_top_k)  -> (topk, all_scored)\n",
    "# PROMPT_TEMPLATE\n",
    "# render_prompt(template, context_str, query_str)\n",
    "# call_llm(prompt, temperature, max_tokens)\n",
    "\n",
    "def build_context(topk_hits):\n",
    "    blocks = []\n",
    "    for i, h in enumerate(topk_hits, 1):\n",
    "        blocks.append(\n",
    "            f\"[{i}] source_file={h['source_file']} chunk_uid={h.get('chunk_uid','')}\\n{h['text']}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def answer_one_question(query: str,\n",
    "                        initial_limit: int = 20,\n",
    "                        final_top_k: int = 5,\n",
    "                        llm_max_tokens: int = 512,\n",
    "                        sleep_sec: float = 0.2):\n",
    "    \"\"\"\n",
    "    單題 end-to-end:\n",
    "    Hybrid retrieval -> rerank -> context -> LLM answer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) Hybrid retrieval\n",
    "        hits = hybrid_retrieve(query, initial_limit=initial_limit)\n",
    "        if not hits:\n",
    "            return \"抱歉，根據目前的資料庫，我無法回答這個問題。\", \"\"\n",
    "\n",
    "        # 2) ReRank\n",
    "        topk, _ = rerank_hits(query, hits, final_top_k=final_top_k)\n",
    "        if not topk:\n",
    "            return \"抱歉，根據目前的資料庫，我無法回答這個問題。\", \"\"\n",
    "\n",
    "        # 3) Build context\n",
    "        context_str = build_context(topk)\n",
    "        chosen_source_file = topk[0].get(\"source_file\", \"\")\n",
    "\n",
    "        # 4) LLM\n",
    "        final_prompt = render_prompt(PROMPT_TEMPLATE, context_str=context_str, query_str=query)\n",
    "        answer = call_llm(final_prompt, temperature=0.0, max_tokens=llm_max_tokens)\n",
    "\n",
    "        # 節流（避免 API 太密）\n",
    "        if sleep_sec > 0:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "        return answer, chosen_source_file\n",
    "\n",
    "    except Exception as e:\n",
    "        # 不讓整批中斷\n",
    "        return f\"【系統錯誤】{type(e).__name__}: {e}\", \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c6e26",
   "metadata": {},
   "source": [
    "# Step 6-C — 批次跑全題目並輸出 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b341f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:23<00:00, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /home/randy/Day6_hw/CW/04/questions_answer_model.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>題目_ID</th>\n",
       "      <th>題目</th>\n",
       "      <th>命中來源文件</th>\n",
       "      <th>模型回答</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>文中提到台中市2月2日當天的氣溫變化範圍為何？</td>\n",
       "      <td>data_01.txt</td>\n",
       "      <td>根據資料，台中市2月2日的氣溫在當天的變化範圍約為 **16 °C（凌晨）至 23 °C（白...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Google Cloud 推出的 N4A 虛擬機器採用哪一款處理器？</td>\n",
       "      <td>data_04.txt</td>\n",
       "      <td>Google Cloud 的 N4A 虛擬機器採用自行研發的 **Google Axion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>根據林氏璧的指出，日本這波流感主要流行哪兩種病毒型別？其占比為何？</td>\n",
       "      <td>data_03.txt</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>LiteRT 的 GPU 加速效能平均比原本的 TensorFlow Lite GPU 委派...</td>\n",
       "      <td>data_05.txt</td>\n",
       "      <td>根據 Google 的測試，LiteRT 的 GPU 加速效能在多種模型的平均情境下，約比既...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>台灣南部與北部的降雨特徵有何主要差異？</td>\n",
       "      <td>data_02.txt</td>\n",
       "      <td>根據資料，台灣北部與南部的降雨特徵主要差異如下：\\n\\n| 項目 | 北部 | 南部 |\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Google Cloud 的 N4A 虛擬機器在今年1月底擴增的區域中，包含哪一個亞洲地區？</td>\n",
       "      <td>data_04.txt</td>\n",
       "      <td>根據資料，擴增的區域中包含亞洲地區 **asia‑southeast1（新加坡）**。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>LiteRT 目前支援哪些平台的 GPU 加速？</td>\n",
       "      <td>data_05.txt</td>\n",
       "      <td>根據目前的資料，LiteRT 的 GPU 加速支援以下平台：\\n\\n- Android  \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>根據日本流感疫情報告，東京目前比較流行的區域有哪些？</td>\n",
       "      <td>data_03.txt</td>\n",
       "      <td>根據日本流感疫情報告，東京目前比較流行的區域包括：\\n\\n- 八王子市  \\n- 町田市  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>針對台中市2月2日的天氣，文中建議民眾該如何穿著？</td>\n",
       "      <td>data_01.txt</td>\n",
       "      <td>根據資料，建議在2月2日於台中市外出時穿著輕薄的長袖衣物，並隨身攜帶一件薄外套，以因應早晚的溫差。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>台灣冬季東北部迎風面多雨，形成「竹風蘭雨」的特色，主要是受到什麼風的影響？</td>\n",
       "      <td>data_02.txt</td>\n",
       "      <td>主要受到冬季吹來的**東北季風**影響。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   題目_ID                                                 題目       命中來源文件  \\\n",
       "0      1                            文中提到台中市2月2日當天的氣溫變化範圍為何？  data_01.txt   \n",
       "1      2                 Google Cloud 推出的 N4A 虛擬機器採用哪一款處理器？  data_04.txt   \n",
       "2      3                  根據林氏璧的指出，日本這波流感主要流行哪兩種病毒型別？其占比為何？  data_03.txt   \n",
       "3      4  LiteRT 的 GPU 加速效能平均比原本的 TensorFlow Lite GPU 委派...  data_05.txt   \n",
       "4      5                                台灣南部與北部的降雨特徵有何主要差異？  data_02.txt   \n",
       "5      6     Google Cloud 的 N4A 虛擬機器在今年1月底擴增的區域中，包含哪一個亞洲地區？  data_04.txt   \n",
       "6      7                           LiteRT 目前支援哪些平台的 GPU 加速？  data_05.txt   \n",
       "7      8                         根據日本流感疫情報告，東京目前比較流行的區域有哪些？  data_03.txt   \n",
       "8      9                          針對台中市2月2日的天氣，文中建議民眾該如何穿著？  data_01.txt   \n",
       "9     10              台灣冬季東北部迎風面多雨，形成「竹風蘭雨」的特色，主要是受到什麼風的影響？  data_02.txt   \n",
       "\n",
       "                                                模型回答  \n",
       "0  根據資料，台中市2月2日的氣溫在當天的變化範圍約為 **16 °C（凌晨）至 23 °C（白...  \n",
       "1  Google Cloud 的 N4A 虛擬機器採用自行研發的 **Google Axion ...  \n",
       "2                                               None  \n",
       "3  根據 Google 的測試，LiteRT 的 GPU 加速效能在多種模型的平均情境下，約比既...  \n",
       "4  根據資料，台灣北部與南部的降雨特徵主要差異如下：\\n\\n| 項目 | 北部 | 南部 |\\n...  \n",
       "5        根據資料，擴增的區域中包含亞洲地區 **asia‑southeast1（新加坡）**。  \n",
       "6  根據目前的資料，LiteRT 的 GPU 加速支援以下平台：\\n\\n- Android  \\...  \n",
       "7  根據日本流感疫情報告，東京目前比較流行的區域包括：\\n\\n- 八王子市  \\n- 町田市  ...  \n",
       "8  根據資料，建議在2月2日於台中市外出時穿著輕薄的長袖衣物，並隨身攜帶一件薄外套，以因應早晚的溫差。  \n",
       "9                               主要受到冬季吹來的**東北季風**影響。  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "from tqdm import tqdm\n",
    "\n",
    "OUTPUT_PATH = Path(\"./questions_answer_model.csv\")\n",
    "\n",
    "# 先試跑幾題（你可以先用 3 或 5，確認沒問題再全跑）\n",
    "DRY_RUN = 10  # 改成 None 代表全跑\n",
    "\n",
    "initial_limit = 20\n",
    "final_top_k = 5\n",
    "\n",
    "rows = df.index.tolist()\n",
    "if DRY_RUN is not None:\n",
    "    rows = rows[:DRY_RUN]\n",
    "\n",
    "for idx in tqdm(rows):\n",
    "    q = str(df.loc[idx, \"題目\"]).strip()\n",
    "    if not q:\n",
    "        continue\n",
    "\n",
    "    ans, src = answer_one_question(\n",
    "        q,\n",
    "        initial_limit=initial_limit,\n",
    "        final_top_k=final_top_k,\n",
    "        llm_max_tokens=256,\n",
    "        sleep_sec=1.0,\n",
    "    )\n",
    "\n",
    "    df.loc[idx, \"模型回答\"] = ans\n",
    "    df.loc[idx, \"命中來源文件\"] = src\n",
    "\n",
    "# 如果你要「直接回填來源文件欄位」，取消註解這行：\n",
    "# df.loc[rows, \"來源文件\"] = df.loc[rows, \"命中來源文件\"]\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Saved: {OUTPUT_PATH.resolve()}\")\n",
    "df.loc[rows, [\"題目_ID\", \"題目\", \"命中來源文件\", \"模型回答\"]].head(len(rows))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
